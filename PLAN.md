# QwenSense: LLM Benchmarking Tool Implementation Plan

## Overview
A dynamic LLM benchmarking tool focused on accuracy rather than speed, designed to test multi-step workflows with tool usage while preventing memorization through randomized entity substitution.

## Core Concepts

### 1. Dynamic Questions
- Template-based questions with placeholders: `{{entity1}}`, `{{entity2}}`, etc.
- Runtime substitution from entity pool (100-200 words: adjectives, nouns)
- Prevents memorization through combinatorial explosion of possibilities

### 2. Dynamic Handling
- Precheck file records actual entity values used for each test run
- JSONL format for easy streaming/appending
- Enables deterministic scoring despite randomization

### 3. Scoring Types
- **readfile_stringmatch**: Read file, exact string match (whitespace trimmed)
- **files_exist**: Check if specified files exist (all must exist)
- **stringmatch**: Traditional Q&A exact string matching
- **directory_structure**: Verify folder hierarchy using path lists
- **tool_call_sequence**: (v2) Capture and validate tool call sequences

### 4. Test Set Definition
- Human-readable YAML/JSON format
- `samples` property: number of randomized runs per question
- Scoring-specific properties (e.g., `file_to_read`, `files_to_check`)

## Project Structure
```
/Users/jvroig/Dev/QwenSense/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ benchmark.py          # Main orchestrator
â”‚   â”œâ”€â”€ entity_pool.py        # Entity word management
â”‚   â”œâ”€â”€ test_runner.py        # Execute tests against LLM
â”‚   â”œâ”€â”€ scorer.py             # Score results using precheck
â”‚   â””â”€â”€ scoring_types/        # Individual scoring implementations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ readfile_stringmatch.py
â”‚       â”œâ”€â”€ files_exist.py
â”‚       â”œâ”€â”€ stringmatch.py
â”‚       â””â”€â”€ directory_structure.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ entity_pool.txt       # Word list for substitution
â”‚   â””â”€â”€ test_definitions.yaml # Human-readable test suite
â”œâ”€â”€ test_artifacts/           # Working directory for LLM file operations
â”œâ”€â”€ results/                  # Output folder
â”‚   â”œâ”€â”€ precheck_TIMESTAMP.jsonl
â”‚   â”œâ”€â”€ responses_TIMESTAMP.jsonl
â”‚   â””â”€â”€ scores_TIMESTAMP.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Implementation Phases

### Phase 1: Core Utilities âœ… COMPLETE
- [x] Entity pool management (`entity_pool.py`)
- [x] Template substitution logic
- [x] Test definition parser (YAML â†’ internal format)
- [x] **BONUS**: Special `{{expected_structure}}` placeholder with tree visualization

### Phase 2: Scoring System âœ… COMPLETE
- [x] Base scorer framework (`scorer.py`)
- [x] Implement scoring types:
  - [x] `stringmatch`
  - [x] `readfile_stringmatch`
  - [x] `files_exist`
  - [x] `directory_structure`
- [x] **BONUS**: Comprehensive result generation with detailed error reporting

### Phase 3: Test Execution
- [ ] Test runner framework (`test_runner.py`)
- [ ] LLM API integration (OpenAI standard, but delegated to external engines)
- [ ] Precheck file generation
- [ ] Response collection

### Phase 4: Integration
- [ ] Main orchestrator (`benchmark.py`)
- [ ] Result aggregation and reporting
- [ ] Error handling and validation

### Phase 5: Testing & Polish
- [x] Sample test definitions
- [ ] Documentation
- [x] Example entity pool
- [x] Usage examples
- [x] **BONUS**: Complete mock testing system

## ðŸŽ‰ **MAJOR PROGRESS UPDATE**

### âœ… **What's Working (Phases 1-2 Complete!)**

**Phase 1 Achievements:**
- **154-word entity pool** with diverse adjectives, nouns, and concepts
- **Dynamic template substitution** with `{{entity1}}`, `{{entity2}}`, etc.
- **YAML test definition parser** with full validation
- **Special directory tree visualization** - `{{expected_structure}}` generates beautiful Unicode trees
- **Complete integration testing** with `system_test.py`

**Phase 2 Achievements:**
- **Full scoring framework** with modular architecture
- **All 4 scoring types implemented and tested**:
  - `stringmatch`: Exact string comparison (trimmed)
  - `readfile_stringmatch`: Read file contents and verify
  - `files_exist`: Check if all specified files exist
  - `directory_structure`: Verify complete directory hierarchy
- **Comprehensive result generation** with JSON output, error details, and statistics
- **Mock testing system** with realistic data and validation

**Current Status:**
- **21 test samples generated** across all scoring types
- **Perfect validation**: 1/21 correct (exactly as expected for mock data)
- **Production-ready scoring system** that can evaluate any LLM responses
- **Anti-memorization working**: Infinite unique question combinations

### ðŸš€ **Ready for Phase 3: Test Execution**

The foundation is rock-solid! We now have:
- âœ… Question generation system
- âœ… Answer validation system  
- âœ… Complete test data pipeline

**Next step**: Build the LLM integration layer to actually run questions against real LLMs and collect their responses.

### ðŸ“Š **System Validation Proof**

- **Generated**: 21 unique questions from 4 templates
- **Scored**: All questions with detailed diagnostics
- **Validated**: Only the exact directory structure we created scored as correct
- **Performance**: 4.76% accuracy (1/21) - exactly what we expected for mock data

## Example Data Formats

### Test Definition (YAML)
```yaml
question_id: 1
samples: 20
template: "Write \"Hey {{entity1}}\" inside of this file (fullpath): test_artifacts/{{entity2}}.txt"
scoring_type: "readfile_stringmatch"
file_to_read: "test_artifacts/{{entity2}}.txt"
```

### Precheck Entry (JSONL)
```json
{"scoring_type": "readfile_stringmatch", "question_id": 1, "sample_number": 1, "template": "Write \"Hey {{entity1}}\" inside of this file (fullpath): test_artifacts/{{entity2}}.txt", "entity1": "rosebud", "entity2": "withered", "file_to_read": "test_artifacts/withered.txt"}
```

### Directory Structure Example
```yaml
scoring_type: "directory_structure"
expected_structure:
  - "{{entity1}}/"
  - "{{entity1}}/{{entity2}}/"
  - "{{entity1}}/logs/{{entity3}}.log"
```

## Design Decisions

### LLM Integration
- Use OpenAI API standard for compatibility
- Delegate actual LLM calling to external engines/APIs
- Focus on testing different tool-calling implementations
- Revisit integration approach during Phase 3

### Anti-Memorization Strategy
- Large entity pool (100-200+ words)
- Multiple samples per question template
- Dynamic substitution at runtime
- Combinatorial explosion makes memorization infeasible

### Scoring Philosophy
- Deterministic and reproducible
- Exact matching (with reasonable whitespace handling)
- Extensible scoring type system
- Clear pass/fail criteria
